{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".h1_cell, .just_text {\n",
    "    box-sizing: border-box;\n",
    "    padding-top:5px;\n",
    "    padding-bottom:5px;\n",
    "    font-family: \"Times New Roman\", Georgia, Serif;\n",
    "    font-size: 125%;\n",
    "    line-height: 22px; /* 5px +12px + 5px */\n",
    "    text-indent: 25px;\n",
    "    background-color: #fbfbea;\n",
    "    padding: 10px;\n",
    "}\n",
    ".code_block {\n",
    "    box-sizing: border-box;\n",
    "    padding-top:5px;\n",
    "    padding-bottom:5px;\n",
    "    font-size: 75%;\n",
    "    line-height: 22px; /* 5px +12px + 5px */\n",
    "    #text-indent: 25px;\n",
    "    #background-color: #fbfbea;\n",
    "    padding: 5px;\n",
    "}\n",
    "\n",
    "hr { \n",
    "    display: block;\n",
    "    margin-top: 0.5em;\n",
    "    margin-bottom: 0.5em;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    border-style: inset;\n",
    "    border-width: 2px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<center>\n",
    "Download and Installation\n",
    "</center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import sys\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "Download the spark tarball in the current directory. The URL is one of many mirrors listed on spark's official website.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_url = \"http://apache.osuosl.org/spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgz\"\n",
    "r = requests.get(spark_url, stream=True)\n",
    "filename = spark_url.rsplit('/')[-1]\n",
    "with open(filename, 'wb') as f:\n",
    "    f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "Extract the spark tarball in the 'spark/' directory.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.call('mkdir spark'.split(' '))\n",
    "subprocess.call('tar -xf spark-2.3.0-bin-hadoop2.7.tgz -C spark --strip-components 1'.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "Add spark_home environment variable.\n",
    "<p>\n",
    "Add spark_home + '/bin' to run a pyspark console.\n",
    "<p>\n",
    "Add spark_home + '/python*' to environment to import pyspark.\n",
    "<p>\n",
    "NOTE: Run this chunk even if you already have Spark installed.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SPARK_HOME'] = os.environ['HOME'] + '/spark'\n",
    "os.environ['PATH'] += ':' + os.environ['SPARK_HOME'] + '/bin'\n",
    "sys.path.append(os.environ['SPARK_HOME'] + '/python')\n",
    "sys.path.append(os.environ['SPARK_HOME'] + '/python/lib/py4j-0.10.6-src.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "Write the names of the slave nodes in the cluster. This script currently assumes the master machine is blue0.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "machines = [\"instance-2\", \"instance-3\", \"instance-4\"]\n",
    "content = \"\\n\".join(machines)\n",
    "with open(os.environ['SPARK_HOME'] + \"/conf/slaves\", 'w') as f:\n",
    "    f.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "We now need to do the same exact thing on all the slave nodes. We will:\n",
    "<ul>\n",
    "<li>\n",
    "Convert this notebook to a python file.\n",
    "<li>\n",
    "Delete the lines after the comment in the code below.\n",
    "<li>\n",
    "Run the editted python script on each slave node.\n",
    "</ul>\n",
    "<p>\n",
    "Note: These nodes must have password-less ssh tunneling configured.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook setup_spark.ipynb to python\n",
      "[NbConvertApp] Writing 5526 bytes to setup_spark.py\n"
     ]
    }
   ],
   "source": [
    "# Convert this ipython notebook to python script\n",
    "!jupyter nbconvert --to=python setup_spark.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file = open('setup_spark.py', 'r')\n",
    "lines = read_file.readlines()\n",
    "read_file.close()\n",
    "\n",
    "del lines[6]  # get_ipython is called for the style chunk, remove this\n",
    "\n",
    "with open('setup_spark.py', 'w') as f:\n",
    "    i = 0\n",
    "    while i < len(lines) and lines[i].strip() != '# Convert this ipython notebook to python script':\n",
    "        f.write(lines[i])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "This will take awhile. For each node, the script is downloading spark, extracting the package and configuring the environment.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ssh instance-2 python < setup_spark.py\n",
    "!ssh instance-3 python < setup_spark.py\n",
    "!ssh instance-4 python < setup_spark.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<center>\n",
    "Using PySpark\n",
    "</center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "Start the cluster.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run start-all.sh\n",
    "subprocess.call(os.environ['SPARK_HOME'] + \"/sbin/start-all.sh\", env=os.environ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "This is where most other programs regarding this project will start.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://instance-1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://instance-1:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f107d611490>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession(SparkContext(master='spark://instance-1:7077'))\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "Lets generate some semi-random data and run a Spark K-Means implementation on it. We'll create a 2D array with 4 centers.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                   A|                   B|\n",
      "+--------------------+--------------------+\n",
      "| 0.09264110029539968| 0.19437044681165994|\n",
      "|0.012536162787798177|  0.2739293070646507|\n",
      "|  0.4902528211318116|0.006829614908638426|\n",
      "| 0.01572251213870518|  0.5741359312992989|\n",
      "|  0.6339949288333049| 0.32263475086230514|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 clusters\n",
    "from random import random, shuffle\n",
    "\n",
    "upper_left = [[random()*0.5, random()*0.5 + 0.5] for _ in range(2500)]\n",
    "upper_right = [[random()*0.5 + 0.5 for _ in range(2)] for _ in range(2500)]\n",
    "bottom_left = [[random()*0.5 for _ in range(2)] for _ in range(2500)]\n",
    "bottom_right = [[random()*.5 + 0.5, random()*0.5] for _ in range(2500)]\n",
    "\n",
    "matrix = upper_left + upper_right + bottom_left + bottom_right\n",
    "shuffle(matrix)\n",
    "\n",
    "data = spark.createDataFrame(matrix, schema=[\"A\", \"B\"])\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                   A|                   B|            features|\n",
      "+--------------------+--------------------+--------------------+\n",
      "| 0.09264110029539968| 0.19437044681165994|[0.09264110029539...|\n",
      "|0.012536162787798177|  0.2739293070646507|[0.01253616278779...|\n",
      "|  0.4902528211318116|0.006829614908638426|[0.49025282113181...|\n",
      "| 0.01572251213870518|  0.5741359312992989|[0.01572251213870...|\n",
      "|  0.6339949288333049| 0.32263475086230514|[0.63399492883330...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vdf = VectorAssembler(inputCols=data.columns, outputCol=\"features\").transform(data)\n",
    "vdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Set Sum of Squared Errors = 416.921486518\n",
      "Centers:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.25413505, 0.25934256]),\n",
       " array([0.25156688, 0.75979991]),\n",
       " array([0.74504744, 0.24207099]),\n",
       " array([0.75325381, 0.74859811])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans(k=4, maxIter=10, initMode=\"random\")\n",
    "model = kmeans.fit(vdf)\n",
    "\n",
    "wssse = model.computeCost(vdf)\n",
    "print(\"Within Set Sum of Squared Errors = \" + str(wssse))\n",
    "\n",
    "print(\"Centers:\")\n",
    "model.clusterCenters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "You can compare this to a python-only single-machine k-means to get an idea of performance gain.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.call(os.environ['SPARK_HOME'] + \"/sbin/stop-all.sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
