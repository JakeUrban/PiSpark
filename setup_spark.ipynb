{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".h1_cell, .just_text {\n",
    "    box-sizing: border-box;\n",
    "    padding-top:5px;\n",
    "    padding-bottom:5px;\n",
    "    font-family: \"Times New Roman\", Georgia, Serif;\n",
    "    font-size: 125%;\n",
    "    line-height: 22px; /* 5px +12px + 5px */\n",
    "    text-indent: 25px;\n",
    "    background-color: #fbfbea;\n",
    "    padding: 10px;\n",
    "}\n",
    ".code_block {\n",
    "    box-sizing: border-box;\n",
    "    padding-top:5px;\n",
    "    padding-bottom:5px;\n",
    "    font-size: 75%;\n",
    "    line-height: 22px; /* 5px +12px + 5px */\n",
    "    #text-indent: 25px;\n",
    "    #background-color: #fbfbea;\n",
    "    padding: 5px;\n",
    "}\n",
    "\n",
    "hr { \n",
    "    display: block;\n",
    "    margin-top: 0.5em;\n",
    "    margin-bottom: 0.5em;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    border-style: inset;\n",
    "    border-width: 2px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<center>\n",
    "Download and Installation\n",
    "</center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import sys\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "Download the spark tarball in the current directory. The URL is one of many mirrors listed on spark's official website.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_url = \"http://apache.osuosl.org/spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgz\"\n",
    "r = requests.get(spark_url, stream=True)\n",
    "filename = spark_url.rsplit('/')[-1]\n",
    "with open(filename, 'wb') as f:\n",
    "    f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "Extract the spark tarball in the 'spark/' directory.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.call('mkdir spark'.split(' '))\n",
    "subprocess.call('tar -xf spark-2.3.0-bin-hadoop2.7.tgz -C spark --strip-components 1'.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "Add spark_home environment variable.\n",
    "<p>\n",
    "Add spark_home + '/bin' to run a pyspark console.\n",
    "<p>\n",
    "Add spark_home + '/python*' to environment to import pyspark.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SPARK_HOME'] = os.environ['HOME'] + '/spark'\n",
    "os.environ['PATH'] += ':' + os.environ['SPARK_HOME'] + '/bin'\n",
    "sys.path.append(os.environ['SPARK_HOME'] + '/python')\n",
    "sys.path.append(os.environ['SPARK_HOME'] + '/python/lib/py4j-0.10.6-src.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "Write the names of the slave nodes in the cluster. This script currently assumes the master machine is blue0.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machines = [\"blue1\", \"blue3\"]\n",
    "content = \"\\n\".join(machines)\n",
    "with open(os.environ['SPARK_HOME'] + \"/conf/slaves\", 'w') as f:\n",
    "    f.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "We now need to do the same exact thing on all the slave nodes. We will:\n",
    "<ul>\n",
    "<li>\n",
    "Convert this notebook to a python file.\n",
    "<li>\n",
    "Delete the lines after the comment in the code below.\n",
    "<li>\n",
    "Run the editted python script on each slave node.\n",
    "</ul>\n",
    "<p>\n",
    "Note: These nodes must have password-less ssh tunneling configured.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert this ipython notebook to python script\n",
    "!jupyter nbconvert --to=python setup_spark.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file = open('setup_spark.py', 'r')\n",
    "lines = read_file.readlines()\n",
    "read_file.close()\n",
    "with open('setup_spark.py', 'w') as f:\n",
    "    i = 0\n",
    "    while i < len(lines) and lines[i].strip() != '# Convert this ipython notebook to python script':\n",
    "        f.write(lines[i])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "This will take awhile. For each node, the script is downloading spark, extracting the package and configuring the environment.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ssh blue1 python < setup_spark.py\n",
    "!ssh blue3 python < setup_spark.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<center>\n",
    "Using PySpark\n",
    "</center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "Start the cluster.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run start-all.sh\n",
    "subprocess.call(os.environ['SPARK_HOME'] + \"/sbin/start-all.sh\", env=os.environ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "This is where most other programs regarding this project will start.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession(SparkContext(master='spark://blue0:7077'))\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "Lets generate some semi-random data and run a Spark K-Means implementation on it. We'll create a 2D array with 4 centers.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 clusters\n",
    "from random import random, shuffle\n",
    "\n",
    "upper_left = [[random()*0.5, random()*0.5 + 0.5] for _ in range(2500)]\n",
    "upper_right = [[random()*0.5 + 0.5 for _ in range(2)] for _ in range(2500)]\n",
    "bottom_left = [[random()*0.5 for _ in range(2)] for _ in range(2500)]\n",
    "bottom_right = [[random()*.5 + 0.5, random()*0.5] for _ in range(2500)]\n",
    "\n",
    "matrix = upper_left + upper_right + bottom_left + bottom_right\n",
    "shuffle(matrix)\n",
    "\n",
    "data = spark.createDataFrame(matrix, schema=[\"A\", \"B\"])\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vdf = VectorAssembler(inputCols=data.columns, outputCol=\"features\").transform(data)\n",
    "vdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(k=4, maxIter=10, initMode=\"random\")\n",
    "model = kmeans.fit(vdf)\n",
    "\n",
    "wssse = model.computeCost(vdf)\n",
    "print(\"Within Set Sum of Squared Errors = \" + str(wssse))\n",
    "\n",
    "print(\"Centers:\")\n",
    "model.clusterCenters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "You can compare this to a python-only single-machine k-means to get an idea of performance gain.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "<center>\n",
    "Latent Semantic Analysis\n",
    "</center>\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "We can look at all the words in a tweet dataset and determine which words may be related to each other, usually by topic.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark doesn't do csv files very well, so we'll read the data into pandas\n",
    "# Then we can pass it to spark\n",
    "if(True):  # if on raspberry pi, change if otherwise\n",
    "    subprocess.call('sudo apt-get install python-pandas'.split(' '))\n",
    "else:\n",
    "    subprocess.call('pip install pandas')\n",
    "\n",
    "import pandas as pd\n",
    "tweets = pd.read_csv(\"https://docs.google.com/spreadsheets/d/1tw90jUqTQoRt-RNOqNWronMN46y7dxb2ciQwj1YFsTo/export?format=csv\")\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.iloc[0]['tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "We need to take each tweet, break it down into words and remove any unuseful words (stopwords)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "from string import punctuation\n",
    "\n",
    "word_punct_tokenizer = WordPunctTokenizer()\n",
    "download('stopwords')\n",
    "swords = stopwords.words('english')\n",
    "\n",
    "def sentence_wrangler(sentence):\n",
    "    word_list = word_punct_tokenizer.tokenize(sentence.lower())\n",
    "    removed_words = []\n",
    "    result = []\n",
    "    for word in word_list:\n",
    "        if word in swords:\n",
    "            removed_words.append(word)\n",
    "            continue\n",
    "        check = False\n",
    "        for char in word:\n",
    "            if char in punctuation:\n",
    "                check = True\n",
    "                removed_words.append(word)\n",
    "                break\n",
    "        if not check: result.append(word)\n",
    "      \n",
    "    return result, removed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "Create a set of all unique words. Then represent each tweet as a RowVector of words.\n",
    "<p>\n",
    "For example, the vector representing \"Clippers down 2 points at the second half #NBAFinals\" would have a 1 in each of the columns of the words in the tweet.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tweets with no special characters (ascii)\n",
    "bag = set()\n",
    "sentences = []\n",
    "for i in range(len(tweets.index[:100])):\n",
    "    t = tweets.iloc[i]['tweet']\n",
    "    try:\n",
    "        t.encode('ascii')\n",
    "        words = set(sentence_wrangler(t)[0])\n",
    "        for word in words:\n",
    "            if word not in bag:\n",
    "                bag.add(word)\n",
    "        sentences.append(words)\n",
    "    except UnicodeDecodeError:\n",
    "        pass\n",
    "bag = frozenset(bag)\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = list(bag)\n",
    "occur_matrix = pd.DataFrame(columns=all_words)\n",
    "zeros = [0]*len(bag)\n",
    "for sentence in sentences:\n",
    "    occur_matrix = occur_matrix.append(pd.DataFrame([zeros], columns=all_words), ignore_index=True)\n",
    "    for word in sentence:\n",
    "        occur_matrix.loc[len(occur_matrix.index)-1, word] = 1\n",
    "occur_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "Now create a co-occurence matrix to represent how many times 2 words appear together in the same dataset. Multiply the occurence matrix by its transpose to get this matrix.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "comatrix = occur_matrix.T.dot(occur_matrix)\n",
    "np.fill_diagonal(comatrix.values, 0)\n",
    "comatrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "Now, this matrix is incredibly sparse, and takes up space. By decomposing this matrix with Sigular Value Decomposition, we can find the K most significant words in the dataset and get the cooccurence matrix of only those words. This significantly reduces the size of the matrix but retains the K most connected words in the data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import DenseVector\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "rows = spark.sparkContext.parallelize([DenseVector(row) for row in comatrix.values.tolist()])\n",
    "row_matrix = RowMatrix(rows)\n",
    "svd = row_matrix.computeSVD(len(comatrix.index)/2, computeU=True)  # reduce matrix by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.call(os.environ['SPARK_HOME'] + \"/sbin/stop-all.sh\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
