{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download spark tarball\n",
    "spark_url = \"http://apache.osuosl.org/spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgz\"\n",
    "r = requests.get(spark_url, stream=True)\n",
    "filename = spark_url.rsplit('/')[-1]\n",
    "with open(filename, 'wb') as f:\n",
    "    f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar -xf spark-2.3.0-bin-hadoop2.7.tgz\n",
      "mv spark-2.3.0-bin-hadoop2.7 spark\n"
     ]
    }
   ],
   "source": [
    "# Extract spark from tarball\n",
    "commands = ['tar -xf spark-2.3.0-bin-hadoop2.7.tgz',\n",
    "            'mv spark-2.3.0-bin-hadoop2.7 spark']\n",
    "i, last_ret_val = 0, 0\n",
    "while i < len(commands) and last_ret_val == 0:\n",
    "    print(commands[i])\n",
    "    last_ret_val = subprocess.call(commands[i].split(' '))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/games:/usr/games:/home/pi/spark/bin:/home/pi/spark/bin\n"
     ]
    }
   ],
   "source": [
    "os.environ['SPARK_HOME'] = os.environ['HOME'] + '/spark'\n",
    "os.environ['PATH'] += ':' + os.environ['SPARK_HOME'] + '/bin'\n",
    "print(os.environ['PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write names of slave nodes to $SPARK_HOME/conf/slaves\n",
    "machines = [\"blue1\", \"blue3\"]\n",
    "content = \"\\n\".join(machines)\n",
    "with open(os.environ['SPARK_HOME'] + \"/conf/slaves\", 'w') as f:\n",
    "    f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook setup_spark.ipynb to python\n",
      "[NbConvertApp] Writing 2031 bytes to setup_spark.py\n"
     ]
    }
   ],
   "source": [
    "# Convert this ipython notebook to python script\n",
    "!jupyter nbconvert --to=python setup_spark.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now remove everything after this in the python file\n",
    "read_file = open('setup_spark.py', 'r')\n",
    "lines = read_file.readlines()\n",
    "read_file.close()\n",
    "with open('setup_spark.py', 'w') as f:\n",
    "    i = 0\n",
    "    while i < len(lines) and lines[i].strip() != '# Convert this ipython notebook to python script':\n",
    "        f.write(lines[i])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot move ‘spark-2.3.0-bin-hadoop2.7’ to ‘spark/spark-2.3.0-bin-hadoop2.7’: Directory not empty\n",
      "tar -xf spark-2.3.0-bin-hadoop2.7.tgz\n",
      "mv spark-2.3.0-bin-hadoop2.7 spark\n",
      "/usr/local/bin:/usr/bin:/bin:/usr/games:/home/pi/spark/bin\n",
      "mv: cannot move ‘spark-2.3.0-bin-hadoop2.7’ to ‘spark/spark-2.3.0-bin-hadoop2.7’: Directory not empty\n",
      "tar -xf spark-2.3.0-bin-hadoop2.7.tgz\n",
      "mv spark-2.3.0-bin-hadoop2.7 spark\n",
      "/usr/local/bin:/usr/bin:/bin:/usr/games:/home/pi/spark/bin\n"
     ]
    }
   ],
   "source": [
    "# run python script on slave nodes, might take awhile\n",
    "!ssh blue1 python < setup_spark.py\n",
    "!ssh blue3 python < setup_spark.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run start-all.sh\n",
    "subprocess.call(os.environ['SPARK_HOME'] + \"/sbin/start-all.sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "PYTHONPATH=$PYTHONPATH:$SPARK_HOME/python\n",
    "PYTHONPATH=$PYTHONPATH:$SPARK_HOME/python/lib/py4j-0.10.6-src.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext(master='spark://blue0:7077')\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
