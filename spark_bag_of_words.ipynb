{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".h1_cell, .just_text {\n",
    "    box-sizing: border-box;\n",
    "    padding-top:5px;\n",
    "    padding-bottom:5px;\n",
    "    font-family: \"Times New Roman\", Georgia, Serif;\n",
    "    font-size: 125%;\n",
    "    line-height: 22px; /* 5px +12px + 5px */\n",
    "    text-indent: 25px;\n",
    "    background-color: #fbfbea;\n",
    "    padding: 10px;\n",
    "}\n",
    ".code_block {\n",
    "    box-sizing: border-box;\n",
    "    padding-top:5px;\n",
    "    padding-bottom:5px;\n",
    "    font-size: 75%;\n",
    "    line-height: 22px; /* 5px +12px + 5px */\n",
    "    #text-indent: 25px;\n",
    "    #background-color: #fbfbea;\n",
    "    padding: 5px;\n",
    "}\n",
    "\n",
    "hr { \n",
    "    display: block;\n",
    "    margin-top: 0.5em;\n",
    "    margin-bottom: 0.5em;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    border-style: inset;\n",
    "    border-width: 2px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "<center>\n",
    "Building a Bag a Words with Spark\n",
    "</center>\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "This notebook assumes you have already ran and understand the code in setup_spark.ipynb. Lets connect to our already installed spark cluster.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SPARK_HOME'] = os.environ['HOME'] + '/spark'\n",
    "os.environ['PATH'] += ':' + os.environ['SPARK_HOME'] + '/bin'\n",
    "sys.path.append(os.environ['SPARK_HOME'] + '/python')\n",
    "sys.path.append(os.environ['SPARK_HOME'] + '/python/lib/py4j-0.10.6-src.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run start-all.sh\n",
    "subprocess.call(os.environ['SPARK_HOME'] + \"/sbin/start-all.sh\", env=os.environ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "Lets first configure the spark environment start the spark jvm application. We can then set some of spark's cluster settings like <i>spark.executor.memory</i>, which controls how much RAM an spark worker process gets. Finally, we can connect to the spark app and get our spark session object.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://instance-6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://instance-6:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc23503b690>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = SparkConf().set('spark.executor.cores', 2).set('spark.executor.memory', '8g')\n",
    "spark = SparkSession(SparkContext(master='spark://instance-6:7077', conf=conf))\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "<center>\n",
    "Why Spark?\n",
    "</center>\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "Building a bag of words requires us to parse every sentence in each essay for every row, identifying unique words and adding them with the correct class label. We'll call this function F. We can speed F up by parallelizing it, i.e. using multithreading. While this may satisfy a need for processing power, it is common to also require large amounts of memory and disk space to apply F or another function to a large dataset, creating a large bag of words.\n",
    "<p>\n",
    "Spark is exellent for this reason. Spark virtualizes both the compute and memory resources of a group of computers (a cluster) and allows users to use the cluster throught a single 'spark' session object.\n",
    "<p>\n",
    "This notebook is a walkthrough of using Spark to apply F (bag-of-words) to a dataset.\n",
    "<ol>\n",
    "    <li>We'll the load dataset into a pandas dataframe.</li>\n",
    "    <li>We'll distribute this dataset across our spark cluster by passing the dataframe to spark.</li>\n",
    "    <li>We'll register a F with spark, allowing spark to call F on each of its worker nodes</li>\n",
    "    <li>We'll use spark to apply F to the distributed dataset.</li>\n",
    "</ol>\n",
    "<p>\n",
    "The result will be a single table of unique words (the joining of the distributed tables) and their class label counts, summarizing the vocabulary used between two different sources.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok here is my where I used the whole table\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "bag_of_words = {}\n",
    "\n",
    "for i,row in donate_table.iterrows():\n",
    "    essay1_sentences = row['project_essay_1'].split('.')\n",
    "    essay2_sentences = row['project_essay_2'].split('.')\n",
    "    title = row['project_title']\n",
    "    label = row['project_is_approved']\n",
    "    \n",
    "    for sentence in essay1_sentences:\n",
    "        words = sentence_wrangler(sentence, swords, legals)[0]\n",
    "        for word in words:\n",
    "            if word not in bag_of_words:\n",
    "                bag_of_words[word] = [0,0]\n",
    "            bag_of_words[word][label] += 1\n",
    "\n",
    "    for sentence in essay2_sentences:\n",
    "        words = sentence_wrangler(sentence, swords, legals)[0]\n",
    "        for word in words:\n",
    "            if word not in bag_of_words:\n",
    "                bag_of_words[word] = [0,0]\n",
    "            bag_of_words[word][label] += 1\n",
    "\n",
    "    words = sentence_wrangler(title, swords, legals)[0]\n",
    "    for word in words:\n",
    "        if word not in bag_of_words:\n",
    "            bag_of_words[word] = [0,0]\n",
    "        bag_of_words[word][label] += 1\n",
    "        \n",
    "    if i%4000 == 0: print('4000 more')\n",
    "            \n",
    "end = time.time()\n",
    "print(end - start)  # roughly 6 minutes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
