{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".h1_cell, .just_text {\n",
    "    box-sizing: border-box;\n",
    "    padding-top:5px;\n",
    "    padding-bottom:5px;\n",
    "    font-family: \"Times New Roman\", Georgia, Serif;\n",
    "    font-size: 125%;\n",
    "    line-height: 22px; /* 5px +12px + 5px */\n",
    "    text-indent: 25px;\n",
    "    background-color: #fbfbea;\n",
    "    padding: 10px;\n",
    "}\n",
    ".code_block {\n",
    "    box-sizing: border-box;\n",
    "    padding-top:5px;\n",
    "    padding-bottom:5px;\n",
    "    font-size: 75%;\n",
    "    line-height: 22px; /* 5px +12px + 5px */\n",
    "    #text-indent: 25px;\n",
    "    #background-color: #fbfbea;\n",
    "    padding: 5px;\n",
    "}\n",
    "\n",
    "hr { \n",
    "    display: block;\n",
    "    margin-top: 0.5em;\n",
    "    margin-bottom: 0.5em;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    border-style: inset;\n",
    "    border-width: 2px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "<center>\n",
    "Building a Bag a Words with Spark\n",
    "</center>\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "This notebook assumes you have already ran and understand the code in setup_spark.ipynb. Lets connect to our already installed spark cluster.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SPARK_HOME'] = os.environ['HOME'] + '/spark'\n",
    "os.environ['PATH'] += ':' + os.environ['SPARK_HOME'] + '/bin'\n",
    "sys.path.append(os.environ['SPARK_HOME'] + '/python')\n",
    "sys.path.append(os.environ['SPARK_HOME'] + '/python/lib/py4j-0.10.6-src.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run start-all.sh\n",
    "subprocess.call(os.environ['SPARK_HOME'] + \"/sbin/start-all.sh\", env=os.environ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "Lets first configure the spark environment start the spark jvm application. We can then set some of spark's cluster settings like <i>spark.executor.memory</i>, which controls how much RAM an spark worker process gets. Finally, we can connect to the spark app and get our spark session object.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://azure4:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://azure4:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f96d6fd7dd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = SparkConf().set('spark.executor.cores', 1).set('spark.executor.memory', '6g')\n",
    "spark = SparkSession(SparkContext(master='spark://azure4:7077', conf=conf))\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "<center>\n",
    "Why Spark?\n",
    "</center>\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "This notebook is a walkthrough of using Spark to build a bag of words from an collection of essays. Spark has 2 core data structures that we operate on: RDD's and DataFrames, corresponding to low and high level libraries respectively. We explore both low-level and high-level Spark implementations.\n",
    "<p>\n",
    "A spark DataFrame is a distributed collection of data grouped into named columns. Notice how this differs from Pandas: <strong>it is not indexed by row</strong>. DataFrames are also strongly typed, either infering the schema from the data or being passed one. This allows spark to bypass type inference later when performing transformations, a costly operation in python. We can still access columns by name, however.\n",
    "<p>\n",
    "<strong>Using DataFrames:</strong>\n",
    "<ol>\n",
    "    <li>Load dataset from a pandas dataframe to a spark dataframe.</li>\n",
    "    <li>Build a bag of words for each row's (project's) text</li>\n",
    "    <li>Sum the counts of matching (word, project approval) keys</li>\n",
    "    <li>Combine the counts of each project approval by (word) keys</li>\n",
    "</ol>\n",
    "<p>\n",
    "A Spark RDD (Resilient Distributed Datasets) is a immutable, partitioned collection of elements that can be operated on in parallel. DataFrames are built on top of RDD's. RDD's do not have column attributes, but can be operated on using the popular map-reduce pattern. We access our RDD from the DataFrame structre we worked with before, allowing us to still access column attributes when writing map or reduce functions.\n",
    "<p>\n",
    "<strong>Using RDD's:</strong>\n",
    "<ol>\n",
    "    <li>We'll break the dataframe into two subtables: rows with label value 0 and rows with label value 1.</li>\n",
    "    <li>We'll distribute each subtable to two nodes. Each will work on its own set of rows, producing a (word, count) dictionary.</li>\n",
    "    <li>We'll combine the counts of matching words into an array.</li>\n",
    "</ol>\n",
    "<p>\n",
    "We'll walk though each step with DataFrames, and will outline using using RDD's in greater detail below.\n",
    "<p>\n",
    "Each data structure has its own advantages, but Spark recommends we work with its higher level DataFrames in Pyspark. Spark even provides a higher level data structure in Scala called DataSets. This is because the enforcement of a schema and indexed columns give dataframe operations space and performance gains. However in Pyspark, RDD's give us control that we lack with DataFrames, and it is helpful to understand what is happening under the hood.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "Lets get the data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = 'https://www.dropbox.com/s/2hdbltrl8bh6kbu/train.csv?raw=1'\n",
    "donate_table = pd.read_csv(url, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182080"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(donate_table.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "Lets get a subset of the columns.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project_essay_1</th>\n",
       "      <th>project_essay_2</th>\n",
       "      <th>project_title</th>\n",
       "      <th>project_is_approved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Most of my kindergarten students come from low...</td>\n",
       "      <td>I currently have a differentiated sight word c...</td>\n",
       "      <td>Super Sight Word Centers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Our elementary school is a culturally rich sch...</td>\n",
       "      <td>We strive to provide our diverse population of...</td>\n",
       "      <td>Keep Calm and Dance On</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hello;\\r\\nMy name is Mrs. Brotherton. I teach ...</td>\n",
       "      <td>We are looking to add some 3Doodler to our cla...</td>\n",
       "      <td>Lets 3Doodle to Learn</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My students are the greatest students but are ...</td>\n",
       "      <td>The student's project which is totally \\\"kid-i...</td>\n",
       "      <td>\\\"Kid Inspired\\\" Equipment to Increase Activit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My students are athletes and students who are ...</td>\n",
       "      <td>For some reason in our kitchen the water comes...</td>\n",
       "      <td>We need clean water for our culinary arts class!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     project_essay_1  \\\n",
       "0  Most of my kindergarten students come from low...   \n",
       "1  Our elementary school is a culturally rich sch...   \n",
       "2  Hello;\\r\\nMy name is Mrs. Brotherton. I teach ...   \n",
       "3  My students are the greatest students but are ...   \n",
       "4  My students are athletes and students who are ...   \n",
       "\n",
       "                                     project_essay_2  \\\n",
       "0  I currently have a differentiated sight word c...   \n",
       "1  We strive to provide our diverse population of...   \n",
       "2  We are looking to add some 3Doodler to our cla...   \n",
       "3  The student's project which is totally \\\"kid-i...   \n",
       "4  For some reason in our kitchen the water comes...   \n",
       "\n",
       "                                       project_title  project_is_approved  \n",
       "0                           Super Sight Word Centers                    1  \n",
       "1                             Keep Calm and Dance On                    0  \n",
       "2                              Lets 3Doodle to Learn                    1  \n",
       "3  \\\"Kid Inspired\\\" Equipment to Increase Activit...                    0  \n",
       "4   We need clean water for our culinary arts class!                    1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "donate_table = donate_table[['project_essay_1', 'project_essay_2', 'project_title', 'project_is_approved']]\n",
    "donate_table.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1148460448"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof(donate_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "<center>\n",
    "1. Load dataset from a pandas dataframe to a spark dataframe.\n",
    "</center>\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "We now have the pandas version of our dataset. We could just pass the dataframe to spark, but spark performs type inference on each value of the dataset to generate a schema if we don't give it one ourselves. So lets create a schema object, and tell spark not to check for itself.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------------------+\n",
      "|     project_essay_1|     project_essay_2|       project_title|project_is_approved|\n",
      "+--------------------+--------------------+--------------------+-------------------+\n",
      "|Most of my kinder...|I currently have ...|Super Sight Word ...|                  1|\n",
      "|Our elementary sc...|We strive to prov...|Keep Calm and Dan...|                  0|\n",
      "|Hello;\\r\\nMy name...|We are looking to...|Lets 3Doodle to L...|                  1|\n",
      "|My students are t...|The student's pro...|\\\"Kid Inspired\\\" ...|                  0|\n",
      "|My students are a...|For some reason i...|We need clean wat...|                  1|\n",
      "+--------------------+--------------------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import ByteType, StringType, StructType, StructField\n",
    "\n",
    "schema = StructType([StructField('project_essay_1', StringType()),\n",
    "                     StructField('project_essay_2', StringType()),\n",
    "                     StructField('project_title', StringType()),\n",
    "                     StructField('project_is_approved', ByteType())])\n",
    "\n",
    "dtdf = spark.createDataFrame(donate_table, schema=schema, verifySchema=False)\n",
    "dtdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "Lets combine the text of all the columns.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|project_is_approved|                text|\n",
      "+-------------------+--------------------+\n",
      "|                  1|Super Sight Word ...|\n",
      "|                  0|Keep Calm and Dan...|\n",
      "|                  1|Lets 3Doodle to L...|\n",
      "|                  0|\\\"Kid Inspired\\\" ...|\n",
      "|                  1|We need clean wat...|\n",
      "+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "dtdf = dtdf.withColumn('text', concat_ws(' ', dtdf.project_title, dtdf.project_essay_1, dtdf.project_essay_2))\n",
    "dtdf = dtdf.drop(*['project_essay_1', 'project_essay_2', 'project_title'])\n",
    "dtdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "<center>\n",
    "2. Build a bag of words for each row's (project's) text\n",
    "</center>\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "We can break each project's text into an array of significant words. But how do we identify when a word is not significant? We can use nltk to tokenize sentences (split into words) and use its stopwords list to filter out insignificant words.\n",
    "<p>\n",
    "We also have to remember that this function will be distributed across the cluster and be performed row by row. We don't want to create a WordPuncTokenizer and download the stopwords for every row in the dataset, so we're going to create them once and broadcast them across the cluster, so each worker node has its own copy.\n",
    "<p>\n",
    "After we create an array of words for each project, we'll create a bag of words for each project, counting how many times each word was used in the list.\n",
    "<p>\n",
    "Make sure nltk is installed on each node in the cluster.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If nltk is not install on worker nodes\n",
    "!ssh instance-7 'pip install nltk'\n",
    "!ssh instance-8 'pip install nltk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jakeu123/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "download('stopwords')\n",
    "swords = stopwords.words('english')\n",
    "\n",
    "# Broadcasting across cluster\n",
    "b_tokenizer = spark.sparkContext.broadcast(WordPunctTokenizer())\n",
    "b_swords = spark.sparkContext.broadcast(swords)\n",
    "\n",
    "def sentence_wrangler(text):\n",
    "    word_list = b_tokenizer.value.tokenize(text.lower())\n",
    "    result = []\n",
    "    for word in word_list:\n",
    "        if word in b_swords.value:\n",
    "            continue\n",
    "        check = False\n",
    "        for char in word:\n",
    "            if char in punctuation:\n",
    "                check = True\n",
    "                break\n",
    "        if not check: result.append(word)\n",
    "    return result\n",
    "\n",
    "def build_bag(words):\n",
    "    d = {}\n",
    "    for word in words:\n",
    "        if word in d:\n",
    "            d[word] += 1\n",
    "        else:\n",
    "            d[word] = 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+\n",
      "|project_is_approved|                text|               words|\n",
      "+-------------------+--------------------+--------------------+\n",
      "|                  1|Super Sight Word ...|[super, sight, wo...|\n",
      "|                  0|Keep Calm and Dan...|[keep, calm, danc...|\n",
      "|                  1|Lets 3Doodle to L...|[lets, 3doodle, l...|\n",
      "|                  0|\\\"Kid Inspired\\\" ...|[kid, inspired, e...|\n",
      "|                  1|We need clean wat...|[need, clean, wat...|\n",
      "+-------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType\n",
    "\n",
    "sentence_wrangler_udf = udf(sentence_wrangler, ArrayType(StringType()))\n",
    "dtdf = dtdf.withColumn('words', sentence_wrangler_udf('text'))\n",
    "dtdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|project_is_approved|                 bag|\n",
      "+-------------------+--------------------+\n",
      "|                  1|[practice -> 3, d...|\n",
      "|                  0|[temporary -> 1, ...|\n",
      "|                  1|[88 -> 1, big -> ...|\n",
      "|                  0|[00 -> 1, educati...|\n",
      "|                  1|[reason -> 1, fee...|\n",
      "+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import MapType, IntegerType\n",
    "\n",
    "\n",
    "build_bag_udf = udf(build_bag, MapType(StringType(), IntegerType()))\n",
    "dtdf = dtdf.withColumn('bag', build_bag_udf('words'))\n",
    "dtdf = dtdf.drop('text', 'words')\n",
    "dtdf.cache()\n",
    "dtdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws, to_json\n",
    "\n",
    "dtdf_str = dtdf.withColumn('freqMap', to_json(dtdf.bag))\n",
    "dtdf.unpersist()\n",
    "dtdf_str.cache()\n",
    "dtdf_str.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bag = dtdf_str.drop('bag')\n",
    "#bag.cache()\n",
    "#dtdf_str.unpersist()\n",
    "#pd_bag = bag.toPandas()\n",
    "pd_bag.to_csv('bag.csv', encoding='utf-8')\n",
    "del pd_bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "<center>\n",
    "3. Sum the counts of matching (word, project approval) keys\n",
    "</center>\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "The explode creates a dataframe by making a row of each key, val pair in the map structure passed to it. We then group by word and project_is_approved, adding the counts of all matching entries.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+----------+\n",
      "|      word|project_is_approved|sum(count)|\n",
      "+----------+-------------------+----------+\n",
      "|      last|                  1|     11566|\n",
      "|       buy|                  1|      3037|\n",
      "|accomplish|                  1|      3465|\n",
      "|   qualify|                  1|      6818|\n",
      "|  teaching|                  0|      4905|\n",
      "+----------+-------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "bagdf = dtdf.select('project_is_approved', explode(dtdf.bag).alias('word', 'count'))\n",
    "bagdf.cache()\n",
    "dtdf.unpersist()\n",
    "groupby_df = bagdf.groupBy('word', 'project_is_approved').agg({'count': 'sum'})\n",
    "groupby_df.cache()\n",
    "bagdf.unpersist()\n",
    "groupby_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "Install pyarrow and pandas across the cluster if you haven't.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyarrow==0.9.*\n",
    "#!ssh instance-7 'pip install pyarrow==0.9.*'\n",
    "#!ssh instance-8 'pip install pyarrow==0.9.*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ssh instance-7 'pip install pandas'\n",
    "!ssh instance-8 'pip install pandas'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "<center>\n",
    "4. Combine the counts of each project approval by (word) keys\n",
    "</center>\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "Now lets groupBy each word again, which will create pandas dataframes containing either 1 or 2 rows. We'll transform this dataframe to a single-row dataframe with the word and its frequency with each project approval value as columns.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| word|counts|\n",
      "+-----+------+\n",
      "|   07|[3, 7]|\n",
      "|14x20|[0, 1]|\n",
      "|1970s|[1, 6]|\n",
      "|  296|[0, 3]|\n",
      "|   3x|[2, 7]|\n",
      "+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "schema = StructType([StructField('word', StringType()),\n",
    "                     StructField('counts', ArrayType(LongType()))])\n",
    "\n",
    "def combine_sums(pdf):\n",
    "    counts = [0, 0]\n",
    "    if len(pdf.index) == 1:\n",
    "        idx = pdf.loc[0].project_is_approved\n",
    "        counts[idx] = pdf.loc[0][\"sum(count)\"]\n",
    "    else:\n",
    "        if pdf.loc[0].project_is_approved == 0:\n",
    "            counts[0] = pdf.loc[0][\"sum(count)\"]\n",
    "            counts[1] = pdf.loc[1][\"sum(count)\"]\n",
    "        else:\n",
    "            counts[0] = pdf.loc[1][\"sum(count)\"]\n",
    "            counts[1] = pdf.loc[0][\"sum(count)\"]\n",
    "    return pd.DataFrame([[pdf.loc[0].word, counts]], columns=['word', 'counts'])\n",
    "\n",
    "cs_udf = pandas_udf(combine_sums, schema, PandasUDFType.GROUPED_MAP)\n",
    "\n",
    "bagdf = groupby_df.groupBy('word').apply(cs_udf)\n",
    "\n",
    "bagdf.show(5)  # 4 Minutes for 100,000 projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|       word|       counts|\n",
      "+-----------+-------------+\n",
      "|      anime|      [5, 60]|\n",
      "|        art|[6862, 30682]|\n",
      "|     biting|      [7, 21]|\n",
      "|   cropland|       [0, 1]|\n",
      "|     ecause|       [0, 1]|\n",
      "|       ells|   [103, 449]|\n",
      "|       holz|       [1, 3]|\n",
      "|       iffy|       [0, 1]|\n",
      "|    jeebies|       [0, 1]|\n",
      "| kleenslate|      [0, 17]|\n",
      "|   mindmash|       [0, 1]|\n",
      "|nexperience|       [3, 8]|\n",
      "| paramedics|       [0, 4]|\n",
      "|     prople|       [1, 0]|\n",
      "|reinfornced|       [0, 1]|\n",
      "|    theough|       [0, 1]|\n",
      "|     voyage|      [2, 28]|\n",
      "| wonderfuls|       [0, 1]|\n",
      "|          ‡ßç|       [0, 1]|\n",
      "|    allowes|       [0, 1]|\n",
      "+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bagdf.cache()\n",
    "groupby_df.unpersist()\n",
    "bagdf.sample(fraction=0.05).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagdf.toPandas().to_csv('bag.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[word: string, counts: array<bigint>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagdf.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "This isn't much faster than the local version, not to mention we don't have the space capacity to compute the entire dataset. The reality is that Spark's advantages are at scale, when workloads are distributed across anywhere from tens to hundreds of nodes. When you have 6 CPU's and have to perform 2 separate groupby operations, Spark's advantages are harder to see.\n",
    "<p>\n",
    "Spark's higher-level DataFrame API provides some optimizations, but doesn't allow for the flexibility of working directly with RDD's. Lets see if we can get rid of one of those groupby's by splitting the dataset across the cluster into 2 separate RDD's based on the value of project_is_approved, applying the groupby to each partition and re-joining the 2 RDD's by combining the counts from both partitions to an array.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+-----+\n",
      "|project_is_approved|          word|count|\n",
      "+-------------------+--------------+-----+\n",
      "|                  1|      practice|    3|\n",
      "|                  1|differentiated|    1|\n",
      "|                  1|          year|    3|\n",
      "|                  1|    considered|    1|\n",
      "|                  1|     obstacles|    1|\n",
      "+-------------------+--------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "freqdf = dtdf.select('project_is_approved', explode(dtdf.bag).alias('word', 'count'))\n",
    "freqdf.cache()\n",
    "freqdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupBy(iterator):\n",
    "    d = {}\n",
    "    for row in iterator:\n",
    "        if row[1].word in d:\n",
    "            d[row[1].word] += 1\n",
    "        else:\n",
    "            d[row[1].word] = 1\n",
    "    yield d.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "Per line:\n",
    "<ol>\n",
    "    <li>create key-value pairs of the rows</li>\n",
    "    <li>split the dataset by key, with one piece on each node</li>\n",
    "    <li>apply groupBy to each partition</li>\n",
    "    <li>re-combine into (word, (count, approval))</li>\n",
    "    <li>combine key (word) pairs and their counts for each approval value</li>\n",
    "    <li>Get the first x</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag = freqdf.rdd.map(lambda x: (x.project_is_approved, x))\\\n",
    "            .partitionBy(2, lambda x: int(x))\\\n",
    "            .mapPartitions(groupBy)\\\n",
    "            .zipWithIndex().flatMap(lambda x: [(y[0], (y[1], x[1])) for y in x[0]])\\\n",
    "            .reduceByKey(lambda x, y: [x[0], y[0]])\\\n",
    "            .take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'fawn', [1, 1]),\n",
       " (u'refreshable', (1, 1)),\n",
       " (u'preparesee', (1, 1)),\n",
       " (u'tah', (1, 1)),\n",
       " (u'comically', (1, 1)),\n",
       " (u'localized', (2, 1)),\n",
       " (u'sevens', (2, 1)),\n",
       " (u'saylor', (1, 1)),\n",
       " (u'sprague', (2, 1)),\n",
       " (u'chatter', [15, 97]),\n",
       " (u'lence', (1, 0)),\n",
       " (u'originality', [11, 29]),\n",
       " (u'alphabetimals', (1, 0)),\n",
       " (u'gaa', (2, 0)),\n",
       " (u'searat', (1, 1)),\n",
       " (u'nreading', [148, 918]),\n",
       " (u'capoeira', (2, 1)),\n",
       " (u'potentiometers', (1, 1)),\n",
       " (u'compuesta', (1, 1)),\n",
       " (u'politician', [1, 11])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "You can see that not all rows have the same format. This is because only a subset of words appeared in essays with both approval values. Those that were only in essays that were approved or denied still have the (word, (count, approval)) format.\n",
    "<p>\n",
    "While I could explore how to fix this, the reality is that this is slower than the previous strategy. I believe this is a result of the fact that we iterate over the dataset and split it into 2 unequal partitions. It is possible CPU resources are being wasted. Our idea of avoiding 2 groupBy's also barely happens because we have to combine the word pairs in the end with a reduceByKey.\n",
    "<p>\n",
    "If you want to give up the free credits and create a larger cluster on Google Cloud, be my guest. Spark is used everywhere in the industry, so I want to see the performance benefits of the tool. For now, I hope the examples within each notebook have gotten you familiar with Spark.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.call(os.environ['SPARK_HOME'] + \"/sbin/stop-all.sh\", env=os.environ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
